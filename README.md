# CharacterCraft: Bridging the Literature-Reality Dialogue Gap for Practical Role-Playing Agents

<p align="center"> <strong>
ğŸ“„ <a href="https://github.com/yin214/CharacterCraft">Paper</a> Â·
ğŸ—‚ï¸ <a href="https://huggingface.co/datasets/alice3214/CharacterCraft-Data">Dataset</a> Â·
ğŸ¤— <a href="https://huggingface.co/alice3214/Qwen2-1.5B-DE">Model</a> 
</strong>
</p>

## Dialogue Extraction

**Qwen2-1.5B-DE** is a specialized language model fine-tuned from `Qwen/Qwen2-1.5B-Instruct` for the specific task of **extracting character dialogues from Chinese literary texts**. The model takes a segment of novel text as input and outputs a structured list of dialogues, complete with speaker identification.

### How to use

You can easily use this model with the `transformers` library.

``` python
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda"

model = AutoModelForCausalLM.from_pretrained(
    'alice3214/Qwen2-1.5B-DE',
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªå°è¯´æ–‡æœ¬åˆ†æé¢†åŸŸçš„ä¸“å®¶ï¼Œä½ éœ€è¦ä»ç»™å®šçš„æ–‡æœ¬ä¸­æå–äººç‰©å¯¹è¯ï¼ŒåŒ…æ‹¬è¯´è¯äººç‰©åç§°å’Œå¯¹è¯å†…å®¹"
user_prompt = '''å°è¯´æ–‡æœ¬:å®ç‰çœ‹ç½¢ï¼Œå› ç¬‘é“ï¼šâ€œè¿™ä¸ªå¦¹å¦¹æˆ‘æ›¾è§è¿‡çš„ã€‚â€è´¾æ¯ç¬‘é“ï¼šâ€œå¯åˆæ˜¯èƒ¡è¯´ï¼ä½ åˆä½•æ›¾è§è¿‡ä»–ï¼Ÿâ€å®ç‰ç¬‘é“ï¼šâ€œè™½ç„¶æœªæ›¾è§è¿‡å¥¹ï¼Œç„¶æˆ‘çœ‹ç€é¢å–„ï¼Œå¿ƒé‡Œå°±ç®—æ˜¯æ—§ç›¸è¯†ï¼Œä»Šæ—¥åªä½œè¿œåˆ«é‡é€¢ï¼Œæœªä¸ºä¸å¯ã€‚â€è´¾æ¯ç¬‘é“ï¼šâ€œæ›´å¥½ï¼Œæ›´å¥½ï¼Œè‹¥å¦‚æ­¤ï¼Œæ›´ç›¸å’Œç¦äº†ï¼â€å®ç‰ä¾¿èµ°è¿‘é»›ç‰èº«è¾¹åä¸‹ï¼Œåˆç»†ç»†æ‰“é‡ä¸€ç•ªï¼Œå› é—®ï¼šâ€œå¦¹å¦¹å¯æ›¾è¯»ä¹¦ï¼Ÿâ€é»›ç‰é“ï¼šâ€œä¸æ›¾è¯»ä¹¦ï¼Œåªä¸Šäº†ä¸€å¹´å­¦ï¼Œäº›é¡»è®¤å¾—å‡ ä¸ªå­—ã€‚â€å®ç‰åˆé“ï¼šâ€œå¦¹å¦¹å°Šåæ˜¯é‚£ä¸¤ä¸ªå­—ï¼Ÿâ€é»›ç‰ä¾¿è¯´äº†åå­—ã€‚å®ç‰åˆé—®è¡¨å­—ã€‚é»›ç‰é“ï¼šâ€œæ— å­—ã€‚â€'''
messages = [
    {"role": "system", "content": prompt},
    {"role": "user", "content": user_prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

Result:
``` text
å®ç‰|è¿™ä¸ªå¦¹å¦¹æˆ‘æ›¾è§è¿‡çš„ã€‚
è´¾æ¯|å¯åˆæ˜¯èƒ¡è¯´ï¼ä½ åˆä½•æ›¾è§è¿‡ä»–ï¼Ÿ
å®ç‰|è™½ç„¶æœªæ›¾è§è¿‡å¥¹ï¼Œç„¶æˆ‘çœ‹ç€é¢å–„ï¼Œå¿ƒé‡Œå°±ç®—æ˜¯æ—§ç›¸è¯†ï¼Œä»Šæ—¥åªä½œè¿œåˆ«é‡é€¢ï¼Œæœªä¸ºä¸å¯ã€‚
è´¾æ¯|æ›´å¥½ï¼Œæ›´å¥½ï¼Œè‹¥å¦‚æ­¤ï¼Œæ›´ç›¸å’Œç¦äº†ï¼
å®ç‰|å¦¹å¦¹å¯æ›¾è¯»ä¹¦ï¼Ÿ
é»›ç‰|ä¸æ›¾è¯»ä¹¦ï¼Œåªä¸Šäº†ä¸€å¹´å­¦ï¼Œäº›é¡»è®¤å¾—å‡ ä¸ªå­—ã€‚
å®ç‰|å¦¹å¦¹å°Šåæ˜¯é‚£ä¸¤ä¸ªå­—ï¼Ÿ
é»›ç‰|æ— å­—ã€‚
```
